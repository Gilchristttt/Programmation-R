---
title: "les test statistiques"
author: "Gilchrist"
format: pdf
editor: visual
---

# Le test du khi-deux: chisq.residuals()

Le test du khi-deux est utilisé pour tester les relations entre des variables qualitatives. Il est souvent employé pour déterminer si des observations sur des variables catégorielles s'écartent significativement de ce que l'on attendait, en fonction d'une hypothèse nulle

L’interprétation des résidus est la suivante :

• si la valeur du résidu pour une case est inférieure à -2, alors il y a une sous-représentation de cette case dans le tableau : les effectifs sont significativement plus faibles que ceux attendus sous l’hypothèse d’indépendance

• à l’inverse, si le résidu est supérieur à 2, il y a sur-représentatation de cette case

• si le résidu est compris entre -2 et 2, il n’y a pas d’écart à l’indépendance significatif

# Test de Student (ou test t): t.test()

Le test de Student est utilisé pour comparer des moyennes entre deux groupes et déterminer si la différence observée entre ces moyennes est statistiquement significative. Ce test s'applique lorsque la variable d'intérêt est quantitative.

# test de Shapiro-Wilk: shapiro.test()

Le test de Shapiro-Wilk est un test statistique utilisé pour évaluer la normalité d'un ensemble de données. Il permet de déterminer si un échantillon donné suit une distribution normale, ce qui est souvent une hypothèse sous-jacente dans de nombreuses analyses statistiques (comme le test t de Student ou l'ANOVA)

#test des rangs de Wilcoxon: wilcox.test

Le test des rangs de Wilcoxon (également appelé test de Wilcoxon-Mann-Whitney pour deux échantillons ou test de Wilcoxon pour un échantillon apparié) est un test statistique non paramétrique utilisé pour comparer des médians ou des distributions entre deux échantillons, ou pour comparer un échantillon à une valeur hypothétique. Contrairement aux tests paramétriques comme le test t de Student, il ne fait aucune hypothèse sur la normalité des données.

# Corrélation linéaire (Pearson): cor()

La corrélation est une mesure du lien d’association linéaire entre deux variables quantitatives. Sa valeur varie entre -1 et 1. Si la corrélation vaut -1, il s’agit d’une association linéaire négative parfaite. Si elle vaut 1, il s’agit d’une association linéaire positive parfaite. Si elle vaut 0, il n’y a aucune association linéaire entre les variables.

# Corrélation des rangs (Spearman) cor(y,y,method = "spearman)

Le coefficient de corrélation de Pearson ci-dessus fait une hypothèse forte sur les données : elles doivent être liées par une association linéaire. Quand ça n’est pas le cas mais qu’on est en présence d’une association monotone, on peut utiliser un autre coefficient, le coefficient de corrélation des rangs de Spearman.

Plutôt que de se baser sur les valeurs des variables, cette corrélation va se baser sur leurs rangs, c’est-à-dire sur leur position parmi les différentes valeurs prises par les variables.

Ainsi, si la valeur la plus basse de la première variable est associée à la valeur la plus basse de la deuxième, et ainsi de suite jusqu’à la valeur la plus haute, on obtiendra une corrélation de 1. Si la valeur la plus forte de la première variable est associée à la valeur la plus faible de la seconde, et ainsi de suite, et que la valeur la plus faible de la première est associée à la plus forte de la deuxième, on obtiendra une corrélation de -1. Si les rangs sont “mélangés”, sans rapports entre eux, on obtiendra une corrélation autour de 0

# Régression linéaire

Une régression linéaire simple se fait à l’aide de la fonction lm()

La régression linéaire est une méthode statistique qui permet de modéliser la relation entre une variable dépendante (ou variable à prédire) et une ou plusieurs variables indépendantes (ou variables explicatives). Elle est utilisée pour prédire la valeur d'une variable en fonction des autres, et pour comprendre les relations linéaires entre les variables.

## Variable dépendante (ou variable à prédire)

La variable dépendante est celle que l’on cherche à expliquer ou à prédire. Elle dépend des variations des autres variables (indépendantes) dans l'étude.

Exemple 1 : Si vous analysez l'impact des heures de révision sur les notes à un examen, la note serait la variable dépendante.

## Variable indépendante (ou variable explicative)

La variable indépendante est celle qui explique ou influence la variable dépendante. Elle est manipulée ou observée pour voir comment elle affecte la variable dépendante.

Exemple 1 : Dans l'exemple précédent, les heures de révision seraient la variable indépendante, car c'est ce qui influence la note à l'examen. \# Tidyverse

Si le data frame d’origine a des rownames, on peut d’abord les convertir en colonnes avec rownames_to_columns: as_tibble(rownames_to_column(mtcars))

À l’inverse, on peut à tout moment convertir un tibble en data frame avec as.data.frame :Là encore, on peut convertir la colonne rowname en “vrais” rownames avec column_to_rownames: column_to_rownames(as.data.frame(d))
